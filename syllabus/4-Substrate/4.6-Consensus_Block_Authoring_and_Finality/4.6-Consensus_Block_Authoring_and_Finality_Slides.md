Lecture 6, Module 4

Instructor: André Silva

---- 

# Consensus

## Block authoring and Finality

In this lecture, we will learn about the consensus protocols implemented in Substrate for block authoring (PoW, Aura and BABE) and finality (GRANDPA)

---

## What is consensus

* The blockchain has a state which is updated by executing the state transition function
* We run the STF by creating new blocks (block execution)

<br/>

* How do we decide who gets to author the next block?
* How do we reconcile diverging views of the current state?
* How can we be sure that a certain state is canonical and will never be reverted?

---

## What do we come to consensus on?

* Consensus is a method for coming to agreement over a shared state
* We want to come to agreement on what is the current canonical block

![Forks](/assets/img/4-Substrate/4.6-forks.png)  <!-- .element height="50%" width="50%" -->

<!-- TODO: this image is a draft -->

Note: the consensus protocols don't care about what the blocks are actually doing, it doesn't care about the STF other than knowing whether it succeeded or not. the protocol is just "blindly" coming to consensus on what block hashes are canonical.

---

## Fork choice

* Given two (or more) different diverging states
* How do we choose which is the canonical fork

Note: we might need to follow multiple forks temporarily, but the fork choice rule also affects immediately the decision on which block to build on top of.

---

## Finality

* Given a certain state / block how certain can we be that it is canonical
    * ... and that it will stay canonical
* If we're certain that a block is canonical we consider it to be **finalized**
* This property can be either:
    * Probabilistic
    * Definite / Provable

Note: the practical notion we're trying to solve here is whether a given block is canonical or not, if it's not canonical it is possible that it might be reverted in the future. Probabilistic finality is akin to eventual consistency, different nodes in the network might have different views of the canonical state at different times, but given enough time (unbound) they should all come to the same conclusion.

---

## Proof of work

<blockquote>
Proof of work (PoW) is a form of cryptographic proof in which one party proves to others that a certain amount of a specific computational effort has been expended.
</blockquote>

* Hard to generate the proof
* But cheap to verify
* Not a consensus mechanism by itself

---

## Nakamoto consensus

* Anyone can extend the blockchain
* To do so requires a proof that a certain amount of work has been completed
* Given a fork always follow the longest chain

---

## Nakamoto consensus

* The PoW is a Sybil deterrence mechanism
  * It is a stochastic process weighted by the relative computational power of participants
* The fork choice rule allows the participants to eventually reach consensus
* Finality is only probabilistic

Note: Nakamoto consensus is eventually consistent so we don't have any definite notion of finality, only probabilistic. As a certain block gets deeper in the history, the probability that it can be reverted gets lower and lower as doing so would require an enormous accumulated amount of computation. Still, it's non-zero.

---

## PoW in Substrate

* Substrate ships with a "nakamoto consensus" implementation
* It doesn't implement any specific PoW algorithm
  * Users of the framework can plug their own
* [Kulupu](https://github.com/kulupu/kulupu) is a project built on substrate that uses PoW-based consensus

Note: https://github.com/paritytech/substrate/tree/master/client/consensus/pow

---

## Moving away from PoW

* PoW-based is very energy intensive
* The work / computation being performed is in itself useless
* We are achieving Sybil resistance by wasting a real-world resource (i.e. energy)

---

## Slot-based block authoring

* Unlike PoW we must have a closed set of authorities with known identities
* We discretize time into slots
* At each slot we have a protocol to decide which of the authorities is entitled to author a block

---

### Aura (authority-round)

* At each slot the authorities are chosen in a round-robin fashion

```
authority(slot) = authorities[slot % authorities.len()];
```

* The chosen authority is allowed to issue no more than one block during that slot
* The longest chain is followed

---

### Aura (authority-round)

* Pros: no wasted energy, simple to implement and to reason about
* Cons: attackable since the full schedule of block production is known in advance
  * You can preemptively attack the next block author to censor it

Note: we can setup a DoS attack on all the authorities as they move from slot to slot preventing them from successfully broadcasting their blocks to the network.

---

### How can we make it behave more like PoW?

* We want to have a slot-based block authoring protocol that has similar properties to PoW
  * Namely that we don't know who the next block author will be
* We want the slot assignment to be a stochastic process
  * ... but without the wasted energy

---

### If we squint we can see an RNG

```
threshold = 1 / number_of_authorities

should_author = rng(seed) < threshold
```

<br/>

* This is similar to what is happening with PoW
  * The threshold is weighted by computation power (unlike in this example)

<br/>

* How can we make sure that the random number generated by an authority was legitimate?
* How can we use randomness in the face of a deterministic execution environment?

---

### Verifiable Randomness Function

<blockquote>
A verifiable random function (VRF) is a public-key pseudorandom function that provides proofs that its outputs were calculated correctly.
The owner of the secret key can compute the function value as well as an associated proof for any input value. 
Everyone else, using the proof and the associated public key can check that this value was indeed calculated correctly.
</blockquote>

<br/>

* Can we use this primitive to assign slots to authorities?

---

### BABE (Blind Assignment for Blockchain Extension)

BABE is a block production mechanism inspired by the [Ouroboros Praos protocol](https://en.wikipedia.org/wiki/Ouroboros_(protocol))

<br/>

* A slot-based block authoring algorithm using a VRF for assignment
  * Nobody knows who the slot author is until they prove it
* A deterministic fallback mechanism
  * Ensuring there will always be a valid slot author

Note: Read the original Ouroboros Praos paper [here](https://eprint.iacr.org/2017/573.pdf)

---

### BABE (Blind Assignment for Blockchain Extension)

* As with Aura we start with a closed set of known authorities
* Time is discritzed into slots, and sets of slots are grouped into epochs

<br/>

* Each slot can have a primary and secondary author (or "slot leader")
  * Primary slots are assigned randomly (using a VRF)
  * Secondary slots are assigned deterministically (with a similar mechanism as in Aura)
* The current slot is a function of "real-world" time
  * `current_slot = current_unix_time / slot_duration`

<div class="right">

![Round Robin Diagram](/assets/img/4-Substrate/4.6-slot-assignment.png) <!-- .element height="40%" width="40%" -->

</div>

Note: In Polkadot, one epoch is 4 hours and on Kusama one epoch is only 1 hour.

---

### Primary slots

* At each slot authorities evaluate the VRF
* VRFs generate a pseudo-random output along with a proof that it was properly generated

```
slot_author(slot, private_key) = vrf(seed, slot, private_key).output < threshold

threshold = 2 ^ vrf_output_length * (1 - (1 - c)) ^ (1 / number_if_authorities)

c is a constant 0 <= c < 1
```

Note: c must be choosen in accordance to the expected maximum network latency and slot duration. it is part of the security model of BABE and outside the scope of this presentation. https://research.web3.foundation/en/latest/polkadot/block-production/Babe.html

---

### Primary slots

![VRF in BABE](/assets/img/4-Substrate/4.6-vrf-in-babe.png)

---

### Seeding the VRF

* As with any RNG the VRF needs to take as input some random material (the seed)
* We need to generate random material in a deterministic manner such that all participants agree on the seed
* We will collect entropy from every VRF output

---

### Seeding the VRF

* As an epoch progresses every VRF output used by a primary slot is stored
* At the end of the epoch the stored data is concatenated
* We hash this data and use it as the seed for a future epoch

<br/>

* TODO - add pseudo-code?

---

### Using the VRF seed

* The randomness computed at epoch N:
  * is announced at the beginning of epoch N + 1 (along with the authorities)
  * and used on epoch N + 2

![Forks](/assets/img/4-Substrate/4.6-vrf-seeding.png)  <!-- .element height="50%" width="50%" -->

<!-- TODO: this image is a draft -->

<br/>

* This guarantees that by the time an authority has registered their intention to validate they don't know what the randomness for the epoch in which they'll be active is

---

### Primary slots

* As with PoW the process of slot attribution is stochastic
* Any slot can be assigned to 0, 1 or more validators
* Having unassigned slots leads to variability in block times
  * Since no authority will be able to produce a block on that slot

---

### Secondary slots

* In order to avoid empty slots we assign all slots to a secondary validator

```
secondary_slot_author = hash(epoch_randomness ++ slot_number) % authorities_len
```

<br/>

* This mechanism guarantees a consistent block time

---

### Fork choice rule for BABE

* Instead of just following the longest chain
* We want to follow the chain that has the most primary slots
  * Since these are the most secure slots
  * Secondary slots can easily be censored (as in Aura)

<br/>

* TODO - Add diagram showing different forks with different number of primary slot blocks

---

## Testing environments

Substrate ships with two tools for running chains in a testing environment:

* [Instant Seal](https://docs.substrate.io/rustdocs/latest/sc_consensus_manual_seal/fn.run_instant_seal.html): creates a new block for every transaction imported into the transaction pool.
* [Manual Seal](https://docs.substrate.io/rustdocs/latest/sc_consensus_manual_seal/fn.run_manual_seal.html): listens for rpc calls to seal blocks and create forks.

TODO: cc @SME - Not sure where to leave this slide - perhaps this can be followed by a quick exercise demonstrating manual and instant seal in action?

---
## BABE vs. PoW

In Proof of Work systems:

* Block authoring is neither slot-based nor requires a known authority set.
* Any mining node with the capacity of solving a computationally complex problem can produce a block at any time.
* The difficulty of this problem can be tuned to provide an statistical target block time.

---

Because multiple validators may be able to produce a block during the same slot, forks are more common in BABE than they are in Aura &mdash; even in good network conditions.

Q: So how does Substrate handle forks?

A: A finality gadget!

---

## Block finality 

Generally, there exists two ways to approach block finality:

1. Probabilistically (eventual): such as with proof of work systems, like Bitcoin and Ethereum 1.x.

2. BFT agreement (immediate): Most finality gadgets are variations of, like Grandpa (GHOST-based Recursive ANcestor Deriving Prefix Agreement).

Note: Have a look at [GASPER](https://ethereum.org/en/developers/docs/consensus-mechanisms/pos/gasper/) for further reading on Ethereum's PoS consensus mechanism.

---

### GRANDPA

GRANDPA is "deterministic" however, there is no such thing as _true_ determinism.
Ultimately, the GRANDPA algorithm decides which set of state changes is final - agnostic to how blocks are produced (which is handled by BABE or Aura).

---

GRANDPA is:

* Fast: it reaches agreements on _chains_ rather than blocks, speeding up the finalization process, even after long-term network partitioning or other networking failures.

* Ensures block finality: as soon as more than 2/3 of validators attest to a chain containing a certain block, all blocks leading up to that one are finalized at once.

* Secured economically: if an attacker wants to take over the chain, they could try, resulting in over 66% of all network tokens getting slashed.

Note: See the original paper for GRANDPA [here](https://arxiv.org/pdf/2007.01560.pdf)

---

### How GRANDPA works

* Finds the highest block number with a sufficient number of votes to be considered final.
    * A node that is designated as the “primary” broadcasts the highest block that it thinks could be final from the previous round.
    *  After waiting for a network delay, each validator broadcasts a “pre-vote” for the highest block that it thinks should be finalized. 

* Chains with the most primary blocks are considered final.
    * If 2/3 of validators are honest, this block should extend the chain that the primary broadcasts. 
    This new chain could be several blocks longer than the last finalized chain.

---

How GRANDPA works (continued): 

* Each validator computes the highest block that can be finalized based on the set of pre-votes. 
If the set of pre-votes extends the last finalized chain, then each validator will cast a “pre-commit” to that chain.

* Each validator waits to receive enough pre-commits to form a commit message on the newly finalized chain.

* GRANDPA supports weighted voting. For example, you could implement GRANDPA on your chain where validators with more stake get more votes.

Note: Source: https://polkadot.network/blog/polkadot-consensus-part-2-grandpa/
In Polkadot all validators have a single, equally weighted vote - an economic decision to prevent small sets of nodes from gaining a large network share.

---

<div class="left">

The black blocks are finalized, and the yellow blocks are not. 
Blocks marked with a "1" are primary blocks; those marked with a "2" are secondary blocks. 
Even though the topmost chain is the longest chain on the latest finalized block, it does not qualify because it has fewer primaries at the time of evaluation than the one below it.

</div>

<div class="right">

![GRANDPA](/assets/img/4-Substrate/4.6-fork-choice.png)

</div>

Note: Introduce GRANDPA's fork choice rule. 
Source of image [here](https://wiki.polkadot.network/docs/learn-consensus#fork-choice).

---

![GRANDPA Basics](/assets/img/4-Substrate/4.6-GRANDPA-basics.png)

Note: GRANDPA figures out which blocks more than 2/3 of nodes have in their chain, and finalizes them. It can also give different nodes different weights. These weights could be determined by amount staked in the protocol. In this diagram, blocks C, B, and A are finalized.

---
## GRANDPA vs. PoW

* PoW gives us probabilistic finality: a block in the past is only as safe as the number of blocks that have been built on top of it.
* As more blocks are built on top of a specific block in a Proof of Work chain, more computational work has been expended behind this particular chain.
* This does not guarantee that the chain containing the block will always remain the agreed-upon chain.

Note: An actor with unlimited resources could potentially build a competing chain and expend enough computational resources to create a chain that did not contain a specific block. 
In such a situation, the longest chain rule employed in Bitcoin and other PoW chains would move to this new chain as the canonical one.

---

## Conflict resolution

* A **safety violation** may occur when two blocks that are in different chains are finalized.
* This implies that at least 1/3 of the validators voted on these two chains.
* GRANDPA has a feature called _accountable safety_ to hold validators accountable for safety violations.

---

Voting on two conflicting chains is called **equivocating**.

GRANDPA detects these "faulty validators" by asking validators the following questions:

1. Why didn’t you consider that one block is final when you voted to finalize the second block?
* Honest validators should answer this with a set of pre-votes or pre-commits for the second round that have a super-majority for the second block.

2. _(assuming that (1) is answered)_ Which pre-votes for the first round have you seen? 
* Honest validators basically rat on other validators and reveal all the votes they received from peers. 
Somewhere in the union of both sets you will discover the validators who voted for the two conflicting chains.

While the _client_ only runs the GRANDPA algorithm, the _runtime_ defines the rules for punishing faulty validators (i.e. when >1/3 of the validators have equivocated).

Note: Specifications for punishing nodes is protocol-specific - Substrates doesn't care - it's in the chain’s business logic, not in the consensus layer.

---

### How can we recover from equivocation?

Scenario: Two different distinct forks have been finalized.

* Some validator casts a double vote (out of incompetence rather than malice).
* Other validators can collect this double vote and report it to the runtime.
* The offending validator will be slashed and the reporting validator will be rewarded.

---

## Common developer interactions and errors

TODO: cc @SME - not sure where we want to go here and in following slides. Please add more content.

---

## How do we decide who the authorities are?

TODO: the authorities are dictated by the runtime (usually through staking). include information about the interaction between the runtime and consensus code, namely that it reads this information through digest data added to the block

---
### Finality Stalls

What happens when more than 1/3rd are offline ?

TODO

---

### Minimum validator requirements

What are the minimum validator requirements ?

TODO

---

### Node Template vs Node vs Parachain Template

TODO 

---

## Workshops and Activities

* [Running into common errors (workshop)](./4.6-Workshops_and_Activities/4.6-Consensus_Block_Authoring_and_Finality_Workshop.md)
* [Illustrate block finality and set-up a private network with block authoring](./4.6-Workshops_and_Activities/4.6-Consensus_Block_Authoring_and_Finality_Activities.md)

---

### Why separate block production and finality?

* **Efficiency**. Consensus systems that make block production part of the finality process make validators send messages with O(n²) complexity for every single authored block: 
    * By isolating block production in a separate module, we can produce blocks in a much more efficient manner.
    * O(n) for BABE and finalize several blocks in one round.

---

* **No constraints on blocks imports.** GRANDPA only requires that the block production system has:
    * Eventual finalization.
    * Follows the fork choice rule of GRANDPA.
    * Block headers have a pointer to its parent block (ensures that light clients can follow the chain).
    * Different failure bounds.

Note: 
On "different failure bounds": With BFT agreement (GRANDPA) you need 2/3 of the validators to be working in order to make progress, if 1/3 of the validators go offline the system will halt. 
With BABE you only need to have 1 validator online to eventually make progress (although at a slower rate).

---
