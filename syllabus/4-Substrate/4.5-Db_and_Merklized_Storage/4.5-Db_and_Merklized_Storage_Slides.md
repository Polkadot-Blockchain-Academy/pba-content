Lecture 5, Module 4

Instructor: Shawn Tabrizi

---- 
# Database and Merklized Storage

In this section, we will learn about how the underlying storage layers in Substrate work and their behavior.

---

# Why is this topic important?

* To understand the main components of Substrate's storage system and how it serves the runtime.
* To make correct decisions when designing new runtime modules.

---

# Storage layers

### High level overview

There are core layers to Substrate's storage system.
```
  ┌────────────────────┐
  │Runtime Storage API │
  ├────────────────────┤
  │Overlay Change Set  │
  ├────────────────────┤
  │Patricia-Merkle Trie│
  ├────────────────────┤
  │Key-Value Database  │
  └────────────────────┘
```

TODO: Add proper diagram of storage layers

---
## Runtime Storage API

* `sp-io` can write to storage with a given key + value
* Substrate has macros that generate APIs to create different storage items as well as read/write to them

---
## Overlay Change Set

<div class="left">

* Stages changes to the underlying DB.
* Overlay changes are committed once per block.

</div>

<div class="right">

Two kinds of changes:

* Prospective Changes - what may happen.
* Committed Changes - what will happen.

</div>

Notes:

Change overlay is part of crate sp-state-machine

---

## Patricia-Merkle Trie

<div class="left">

* [paritytech/trie](https://github.com/paritytech/trie)
* Data structure on top of KVDB
* Arbitrary Key and Value length
* Nodes are Branches or Leaves

TODO: review this

</div>

<div class="right">

![Diagram of trie](http://placehold.jp/150x150.png)

</div>

---

## Key-Value Database (KVDB) Layer

<div class="left">

* Implemented with RocksDB and Parity DB
* Just KV mapping: `Blake2_256` Hash -> `Vec<u8>`

</div>

<div class="right">

| Key (Hash) | Value (`Vec<u8>`) |
| -----| --------| 
| 0x0fd923ca5e7 | [00] |
| 0x92cdf578c47 | [01] |
| 0x31237cdb79 | [02] |
| 0x581348337b | [03] |

Note: There could also be some in-memory KVDB layer for testing purposes.

Memory kvdb is also notably used as a triedb backend during proof verification.

---

### RocksDB

A Persistent Key-Value Store for Flash and RAM Storage.

* Keys and values are arbitrary byte arrays
* Fast
* Secure

Note:
See http://rocksdb.org/.
Big project, can be very tricky to configure properly. (also a big part of substrate compilation time).

---

### ParityDB

An Embedded Persistent Key-Value Store Optimized for Blockchain Applications.

* Designed for efficiently storing blockchain state encoded into a Patricia-Merkle trie
* Optimized write performance

Note:

Main point is that paritydb suit the triedb model.
Indeed triedb store encoded key by their hash.
So we don't need rocksdb indexing, no need to order data.
Parity db index its content by hash of key (by default), which makes access faster (hitting entry of two file generally instead of possibly multiple btree indexing node).
Iteration on state value is done over the trie structure: having a kvdb with iteration support isn't needed.

Both rocksdb and paritydb uses "Transactions" as "writes done in batches". 
We typically run a transaction per block (all in memory before), things are fast (that's probably what you meant).
In blockchains, writes are typically performed in large batches, when the new block is imported and must be done atomically.
See: https://github.com/paritytech/parity-db

Concurrency does not matter in this, paritydb lock access to single writer (no concurrency).
Similarily code strive at being simple and avoid redundant feature: no cache in parity db (there is plenty in substrate).

'Quick commit' : all changes are stored in memory on commit , and actual writing in the WriteAheadLog is done in an asynchronous way.

---

### Two Kinds of Keys

1. Trie key path 
2. KVDB key hash


Note:

Storage access path (corresponding to the `Runtime Api` level of the overview slide) can be seen as a third kind (module, storage structure and possibly key in strorage structure) (translate to trie key that then query kvdb hash).

---
## Cost to Read and Write

* Runtime -> overlay is cheap, etc.
* Where are the bottlenecks?
* Don’t have to pay the hashing or disk access

TODO: ???

Note:

Hashing cost is only when calculating the root (always end of block but can happen mid block if needed).

Generally the disk access cost is after block acceptance, it will have some impact but is rather asynch.

---

## Database Backend

* Trie DB
* Trie Root 
* Storage Proofs

---

## Patricia-Merkle Trie

[Parity's Trie implementation](https://github.com/paritytech/trie) provides a generic implementation of the Base-16 Modified Merkle Tree datastructure.

<div class="left">

* Substrate uses a Base-16 Patricia Trie
* Merkle tree allows you to easily prove that some data exists within the tree with a “Merkle Proof”.

</div>

<div class="right">

![Merkle tree](/reveal-md/assets/4.5/merkle-tree-1.png)

TODO: update image design.

</div>

---
### Merkle tree 

<div class="left">

* Root node: can be used to verify two trees are the same.
* Branch nodes
* Leaf nodes

</div>

<div class="right">

![Merkle tree](/reveal-md/assets/4.5/merkle-tree-1.png)

TODO: update image design.

</div>

---

### Patricia trie 

<div class="left">

* Position in the tree defines the associated key.
* Space optimized for elements which share a prefix. 

</div>

<div class="right">

![Merkle tree](/reveal-md/assets/4.5/patricia-trie-1.png)

TODO: update image design.

</div>

---

* Patricia provides the trie path.
* Merkle provides the recursive hashing of children nodes into the parent.
* The Trie key path is set by you, for e.g. `:CODE`.
* A trie node has arbitrary length containing a header, key, possible children, possible value.
* KVDB key = Hash([Trie Node])

---

## Anatomy of a node

Radix tree node:
```
  [partialkey ++ n children ++ maybe_value]
```

Merklized:
```
  [partialkey ++ n children hash ++ maybe_value]
```
Node encoding variants:
```
- [Header branch ++ partialkey ++ maybe_value
  ++ n children hash]
- [Header leaf ++ partially ++ value hash]
- [Value]
```

Notes:
- radix

Add constraint that terminal node always contains a value

- merklized

Note that merklizing this pretty common datastructure just means replacing/enriching the pointers to child with a hash.

Then of course persisting nodes in some way and lazy loading by hash (unless keeping all in memory or rebuilding on flight: not substrate).

variant should be as compact as possible.

Additional subtleties that if content under hash is smaller than it s hash, then it is written in the parent node instead of hash
-> for either value or children

## Complexity

* Reading
* Writing 

---

## Reading

* O(log n) reads

![Merkle tree](/reveal-md/assets/4.5/complexity-storage-reads.png)

TODO: Update diagram with design.

---
## Writing

<div class="left">

* Very expensive for a database
* O(log n) reads, hashes and writes

1. Follow the trie path to the value.
    * O(log n) reads
2. Write the new value.
    * 1 write
3. Calculate new hash
    * 1 hash
4. Repeat (2) + (3) up the trie path
    * O(log n) times

</div>

<div class="right">

![Merkle tree](/reveal-md/assets/4.5/complexity-storage-reads.png)

</div>

TODO: Update diagram with design (modify accordingly)

---

## Multiple state

- from multiple blocks
TODO: diagram from storage deep dive with multiple block

- child trie

Notes:
between two blocks trie nodes are shared and the storage structure of the trie inherently allow storing history of index.

really multiple indexing (by the root and the trie) allows storing history for each block: that is a very convenient model.

---

## Merkle Proof

<div class="left">

* O(log n)
* Great for light clients!
* Low bandwidth, low computation

</div>

<div class="left">

1. Full Node: Follow the trie path to the value.
    * O(log n) reads
2. Full Node: Upload data of trie nodes.
3. Light Client: Download trie node data.
4. Light Client: Verify by hashing.
    * O(log n) hashes

</div>

Notes:

- Message is that proof is just enough trie content (can be a bag of node or some ordered node that needs to be complete with hashing as in compact proof TODO should we make a slide for compact proof and generally proof serialization?) to build a subset/subpart of the full state trie.

This incomplete trie will then be accessed and used identically as the full state trie, but if access is not part of the proof, then the action is not finishing: Proof Incomplete case.

Invalid proof are proof where the hashing don't match (can be see as multiple trie).

TODO Could have some schema with the full state, then the proof and then two query on the proof: one that access data available and one that fail because incomplete : Probably already exists in storage deep dive


---

## Recording proof

- Simple footprint of the node accessed
- Plus removing redundant hashes (compact proof)
- Read access, write access, iteration
- Beware of cache

Notes:

Another message to convey is that producing proof is really only recording all access made during some actions (key access, value insert, value change, trie iteration...).
Any kind of changes work.

This could be extended by the idea that key value caching should be disable for the first action otherwhise trie node would not be access and we would not register proof correctly.
-> can extend to basti pr where there is two kind of cache: trie node level cache that is safe to use and key value cache that 
Not sure it is worth going to far on cache strategy, but may be relevant to mention that by its structure trie node cache is shared between block.


---

## Storage and proof size.

- Hexary trie good for disk storage
- Binary trie smaller foot print in proof
- Both concerns could be separated

Notes:

The trie structure (hexary) is mostly related to the storage model and do not produce the more compact proofs. One direction would be to decorelate storage from merklization. eg hexary node in storage but merklization over binary node. But the model get more complex. 

A final message to it should be that (eth see it), the storage model is still not the most efficient: we use merkle trie index to access node that are stored under a btree index (rocksdb), a true state db would have it's inner indexing directly using the merkle structure.
Paritydb in this sense in a good middle ground as it implement a hash map access directly so the merkle trie index is over a hash map rather than a btree map: that is a huge gain.

What works in memory as simple data structure, also work as a db over disk and also extend to being merklized. Usually things can be mapped or reffered to rather naturally. For instance an optimisation of radix trie is not storing the full merkle path in each node and get the key with the value: this work in memory (not a huge gain), this work on disk (huge gain as you can have fix len node which is big gain for disk access), can work with merkle proof (but tricky if codec still store the full partial key).


---
## Exercise: discuss performance optimizations 

TODO: ask questions that relate to the work done here: https://github.com/paritytech/trie/pull/157 and https://github.com/paritytech/trie/pull/142 for students and lecturer to engage about optimizing state reads. Then look over the PR's motive.

---

## Overlay Deep dive

The overlay stages changes to the underlying database.

---

### Optimizations

* Minimize backend writes
* Minimize calculating storage root
* Only store consensus critical data in your runtime storage

---

### Balancing trie

- Needed for contract
- Not needed for runtime

Notes:

Here (or somewhere else) it must be evoked that trie path (key for values) are whatever the runtime want.
This is a very important design consideration:
in ethereum for instance everything is stored under hash(key), which makes the trie balanced amongst all value.
in substrate we allow random length key (there is a limit but very high in the trie impl), because the runtime
can be responsible of trie unbalance.
A slide showing an unbalance trie could be nice:
- a branch with module balance prefix and a lot of balance behind: making the query cost like 2 nodes for prefix and let's say 100_000 account so 16^5 -> ~ 5 nodes (accounts are hash and under the prefix things are balanced). -> 7 nodes to access
- a branch with some random 
odule and a constant in it : 2 nodes for prefix, + 2 nodes to access the constant. -> 4 nodes to access
- the wasm runtime at :code -> only 2 nodes

So choice of key (even if mainly made for you by the runtime storage macro) is very important.
And having an unbalance may sound like a bad thing, but it is not.

TODO could be moved as a slide before.

---

Substrate ships with additional ["transactional" overlays](https://github.com/paritytech/substrate/blob/master/frame/support/src/storage/transactional.rs).

This provides an API for developers to write logic with multiple storage writes in a single transaction, where:
* Either the entire changes to storage are committed..
* Or nothing is committed at all.

Note:
With this storage overlay, you don't need to verify each and every storage access before doing a modification.
In module 6, we can take a closer look at how this functionality is enforced for all FRAME extrinsics (see: https://github.com/paritytech/substrate/pull/11431).

---

## How does the Runtime sees state?

* `root()`, `get()`, `set()`, `next_key()`
* runtime sees state as a ordered Key-Value
* this Key-Value in the client is a Merkle tree

---

## Manual pruning exercise

Let's demonstrate what pruning looks like for retaining only the old Merkle Trie nodes.

---

## Workshop and Activity

* [Database and Merklized Storage Workshop](./4.5-Workshops_and_Activities/4.5-Db_and_Merklized_Storage_Workshop.md)
* [Database and Merklized Storage Activity](./4.5-Workshops_and_Activities/4.5-Db_and_Merklized_Storage_Activities.md)
