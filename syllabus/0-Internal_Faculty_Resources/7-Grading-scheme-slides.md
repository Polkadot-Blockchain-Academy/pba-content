---
title: New Academy Grading Scheme
description: Research and proposal from Nuke
duration: 30 minuets
---

<!-- .slide: data-background-color="#4A2439" -->

# New Academy Grading Scheme

---

## 🎯 Goals (1)

**Objective, easy to concretely define achievement, and concrete measures on assessment - qualitative and quantitative.**

- Grading to be uniform and transparent for _everyone_
- Maps _exactly_ with learning goals & outcomes (more on this latter)
- Rubrics and complete solutions (A "Perfect" one, and ideally various levels of score categories as example for each) for all graded material
- Test suites for code to check functionality
- "fuzzy" topics like code quality & "beauty" need to be confined outside of hard skills & competencies in any assessments/scores/pass or fail

---v

## 🎯 Goals (2)

**Focus on growth & learning, not a number for grades**

- [Learning, not earning! 📺](https://www.youtube.com/watch?v=CnSkOXe90WI)
- Foster intrinsic motivation & drive
- No points system can be high "rigor", focus must be on quality of work
- Encourages unique work & creative thinking
- Better granularity via skills and competencies, vs. "some number" that encompasses overall growth/accomplishment

---v

## 🎯 Goals (3)

Minimize time and effort needed to assess work during a cohort.

- Time on site is VERY limited in the PBA, and we would rather have students and faculty interacting face-to-face instead of async reviewing and grading
- Certifications (for now) are awarded at the end of the cohort, and thus the absolute deadline is before the end of the program for _all grades_
- Minimal turn-around time to get actionable feedback will enable students seek support within the cohort from SMEs, rather than async after the cohort

---v

## 🎯 Goals (4)

We must balance between:

1. Providing detailed feedback from SME to all students on all graded assignments
   - (At least option to request this, this can be TA's primary role)
1. Minimizing time and effort needed to assess work during a cohort

---v

## 🎯 Goals (5)

- Dry runs and/or beta testing to be complete before students are tasked with any materials
- Flexibility to break "out of the mold" to follow passions & dig deep
- Enable a pathway to thesis based model for multi-track future with tools and techniques used
  - "Certifications" granted at the closing ceremony are for participation only
  - _The exception may be the Application Engineers (parachain and solochain engineering) for completion of some level in all assignments_
  - Only post-cohort will specific distinctions (engineering, founder, etc.) be granted a proper degree/certification

---v

## 🙅 Anti-goals

- "Shame" under-achievement that leads to exiting the ecosystem.
  - _We want to Maximize continued high-impact involvement post academy for all students and alumni_
- Normalization for the sake of specific percent not certifying
- "Gossip" and personal gripes/bias about students leading to bias & keep (by default, without good cause) _any_ subjective details on students out of the picture _between graders_.
  - _Thesis committee would break this rule, running after in-person cohort is complete_
- Unclear and subjective points systems with no clear definition that leads to grader interpretation of what a score means

---

<!-- .slide: data-background-color="#4A2439" -->

# Alternative Grading Frameworks

---v

## Alternative Grading Frameworks

- Overview: [📰 UC Berkeley Alternative Grading Frameworks](https://teaching.berkeley.edu/resources/course-design-guide/design-effective-assessments/alternative-grading-frameworks)
- Also [📰 University of Miami Alternative Grading Frameworks](https://academictechnologies.it.miami.edu/explore-technologies/technology-summaries/alternative-grading/index.html)
- There are many other interesting but even less common frameworks, not discussed in this presentation.

---v

## [📰 Grading for Equity](https://www.insidehighered.com/views/2020/01/27/advice-how-make-grading-more-equitable-opinion)

- Subjective criteria to minimum or zero
- Transparent scoring -> Are mathematically accurate to validly describe a student’s level of mastery
- No normalization
- Support hope and a growth mind-set
- “Lift the veil” on how to succeed

Notes:

- Transparent scoring -> Are mathematically accurate to validly describe a student’s level of mastery
  - They apply a more proportionately structured 0-4 scale instead of the 0-100 scale, which is mathematically oriented toward failure
  - They also use sound mathematical principles that reflect recent performance and growth instead of averaging performance over time
- Support hope and a growth mind-set
  - They allow test/project retakes to emphasize and reward learning rather than penalize it, and they override previous scores with current scores that build learning persistence
- “Lift the veil” on how to succeed
  - They create explicit descriptions of what constitutes demonstration of content mastery through rubrics or proficiency scales.
  - In addition, they simplify grade books and expand the methods of assessments to generate more accurate feedback and reporting about each student’s learning relative to the expected outcomes

---v

## [📰 Specs Grading](https://www.insidehighered.com/views/2016/01/19/new-ways-grade-more-effectively-essay)

- Pass fail only (like PRs!)
- Bundles to achieve levels

---v

## [📰 Ungrading](https://www.insidehighered.com/advice/2017/11/14/significant-learning-benefits-getting-rid-grades-essay)

- Reflection and Dialogue: Ungrading builds upon similar aspects of specifications and contract grading
  - Assignments provide clear instructions, although not necessarily criteria or contracts, for students to follow
  - Instructors should also offer students flexibility with assignment deadlines and provide opportunities for revision
  - Ungrading does encourage instructors to have more open conversations with students about their performance, whether it is through bi-weekly conferences, feedback surveys, or asking students outright what grade to put in the system at the end of the term (Blum & Kohn, 2020)
  - These conversations in addition to other self-reflective exercises (i.e. minute tickets, process letters, peer feedback, etc.) require students to think critically about what they’ve learned and articulate how they have developed their knowledge and skills throughout the semester

---v

## [📰 Contract Grading](https://en.wikipedia.org/wiki/Contract_grading)

- Collaboratively define grade qualifications with students (not completely pre-defined)
- Completion of a _contracted_ number of assignments of specified quality that correspond to specific letter grades
  - Instructors and students know exactly what is expected from them to receive a certain letter grade (no normalization)
  - Any student who completes the work that corresponds to a "B" grade will receive a "B" (everyone can pass)
- The grade the student receives is a reflection of how well they completed the pre-determined syllabus.

- _Variant: Labor-based contract grading = writing assessment be based on effort rather than on a subjective evaluation_

---v

## [Competency Grading 📺](https://www.youtube.com/watch?v=YQInjf8UjOo)

Great explainer in [three](https://www.nciea.org/blog/what-do-i-need-to-know-about-competency-based-grading) - [part](https://www.nciea.org/blog/what-do-i-need-to-know-about-competency-based-grading-2) - [serries](https://www.nciea.org/blog/what-do-i-need-to-know-about-competency-based-grading-3)

---

<!-- .slide: data-background-color="#4A2439" -->

# PBA's (proposed) Grading Scheme

---v

## PBA's (proposed) Grading Scheme

- Scoring System
- Certification Criteria
- Automated Grading Framework
- PR Reviews for Feedback

---v

## 📊 Scoring System

Ala specification grading we use a **0 to 4 integer system** per assignment

- 0 = Nothing submitted or grossly poor performance
- 1 = Incomplete submission, under minimal requirements
- 2 = Passing all minimal requirements, pre-defined percent of test suite passing
- 3 = Passing all test suite items, including explicitly marked optional ones
- 4 = Grader discretion of going above the call of the assignment
  - Taking into account the level expected of students in context
  - Examples of what should qualify defined by creator(s)

---v

## ✅ Certification Criteria

Options for criteria for certification:

1. Average of integer scores for all assignments
1. Bundling of assignments that must be above a 2 to get cert
   - pre-defined
   - contract negotiated
1. Thesis Defense (only)

---v

## 🤖 Automated Grading Framework

Based on Joshy's awesome work on the [📑 qualifying exam](https://github.com/Polkadot-Blockchain-Academy/Rust-Entrance-Exam) and [📑 assignment 0](https://github.com/Polkadot-Blockchain-Academy/pba-pre-course-assignment) we will strongly suggest all (Rust based) assignments follow this standard:

<pba-flex center>

- Templated starting point with faculty to craft cencrete assigments with `todo!("some things here...")` skeleton code defined
- Include "sanity check" and minimal unit tests that students _must_ complete
- Automated test suite (closed/private) to score granular pass/fail (based on learning objectives)

</pba-flex>

---v

## 🕵️ PR Reviews for Feedback (1)

- A student README that discusses the work that SME will review
  - calls out things the student wants the SME to review in work directly
  - reflection on the work: key learning, things still to do, unresolved questions/issues the student had
- Unit tests / CI to run and flag pass/fail (like qualifier)
- Classrooms feedback PR can be used to comment directly on the work of each student easily
  - Issue template is the grading rubric with tasks complete or incomplete to define if competency is met....? (next slide)

---v

## 🕵️ PR Reviews for Feedback (2)

- Competency checklist
  - Competencies are for the course, not the assignment - there will be overlap over course, not locked per assignment

---

<!-- .slide: data-background-color="#4A2439" -->

# Assignment and Grading<br>Resources for Faculty

---v

## [📺 Developing Quality Assessments](https://www.youtube.com/watch?v=CnSkOXe90WI)

- Blooms taxonomy as basis with _explicit_ terms to use when defining learning objectives and outcomes
- WHY give this assignment? in context with content
- We assess on:
  - Process (thinking through) and/or product (shipped solution {code})
  - Express ideas concisely and coherently
  - Convergent (coming to conclusion based on given) or divergent (hypothesis from predictions & unstated things)

---

## [📺 Best Practices for Grading<br>Objectively and Efficiently](https://www.youtube.com/watch?v=hiUXBr4sgnM)

> Note that this is for typical grading systems (out of 100, A->F) that we will _not_ employ

---

<!-- .slide: data-background-color="#4A2439" -->

# Thesis Driven Certification Model

---v

## Thesis Driven Certification Model

PBA is already a "mini-masters" program, this would expand that analogy formally:

1. Select Thesis Advisor, student & advisor mutually agree on relationship
1. Thesis Proposal: defines work to be done, approved by Advisor (and retries if needed)
1. Thesis work: async support from Advisor along the way
1. Thesis Committee: define a cohort of peers and SMEs to evaluate thesis
1. Thesis Defense: Thesis Committee to critically review work, and deem if worthy of a degree
1. Certification Awarded
